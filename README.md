# ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„

CIFAR10 ë°ì´í„°ì…‹ì— ëŒ€í•œ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ í”„ë¡œì íŠ¸

---

## ğŸš€ ê°œìš”

ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŠ¹ì • ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë”¥ëŸ¬ë‹ ê¸°ë°˜ì˜ ë¶„ë¥˜ ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í–ˆìŠµë‹ˆë‹¤. í•™ìŠµ ê³¼ì • ë‚´ë‚´ ì •í™•ë„ëŠ” ê¾¸ì¤€íˆ í–¥ìƒë˜ì—ˆê³ , ì†ì‹¤ ê°’ì€ ì•ˆì •ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” ëª¨ìŠµì„ ë³´ì—¬ ì„±ê³µì ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

---

## ğŸ“‹ í•™ìŠµ í™˜ê²½ ë° ì„¤ì •

- í”„ë ˆì„ì›Œí¬: PyTorch
- ìµœì í™” ë„êµ¬ (Optimizer): Adam
- ì†ì‹¤ í•¨ìˆ˜ (Loss Function): CrossEntropyLoss
- í•™ìŠµë¥  (Learning Rate): 0.001
- ì—í¬í¬ (Epochs): 10
- í‰ê°€ ì§€í‘œ: ì •í™•ë„ (Accuracy), ì†ì‹¤ (Loss)
- ë°ì´í„°ì…‹: í•™ìŠµ(Training) ë°ì´í„°ì™€ ê²€ì¦(Validation) ë°ì´í„° ë¶„ë¦¬

---

## ğŸ“ˆ í•™ìŠµ ê²°ê³¼

### í•™ìŠµ ê²°ê³¼ í‘œ
| Epoch | Train Loss | Train Accuracy | Validation Loss | Validation Accuracy |
|-------|------------|----------------|-----------------|---------------------|
| 1     | 1.4599     | 46.89%         | 1.1647          | 58.52%              |
| 2     | 1.0813     | 61.24%         | 0.9322          | 66.82%              |
| 3     | 0.9605     | 65.93%         | 0.9551          | 66.04%              |
| ...   | ...        | ...            | ...             | ...                 |
| 10    | 0.6959     | 76.01%         | 0.6126          | 78.46%              |

### Accuracy and Loss Trends
#### Accuracy
- í•™ìŠµ ì •í™•ë„ëŠ” Epochë§ˆë‹¤ ìƒìŠ¹í•˜ì—¬ ìµœì¢… Epochì—ì„  **76.01%**ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.
- ê²€ì¦ ì •í™•ë„ ë˜í•œ ë¹„ìŠ·í•œ ì¶”ì„¸ë¡œ, **78.46%**ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.
- ëª¨ë¸ì´ íŠ¹ì • ë°ì´í„°ì— ê³¼ì í•©ë˜ì§€ ì•Šê³ , ìƒˆë¡œìš´ ë°ì´í„°ì—ë„ ì˜ ì¼ë°˜í™”ë  ìˆ˜ ìˆìŒ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

#### Loss
- í•™ìŠµ ì†ì‹¤ì€ ì§€ì†ì ìœ¼ë¡œ ê°ì†Œí•˜ë©° 0.6959ê¹Œì§€ ë‚®ì•„ì¡ŒìŠµë‹ˆë‹¤.
- ê²€ì¦ ì†ì‹¤ë„ ë§ˆì°¬ê°€ì§€ë¡œ ê°ì†Œí•˜ì—¬ 0.6126ìœ¼ë¡œ ë‚®ì•„ì¡ŒìŠµë‹ˆë‹¤.
- ëª¨ë¸ì´ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

#### Training VS Validation
![Plot](./outputs/output.png)

---

## ğŸ” Analysis

### Key Observations
1. **Performance**:
   - Steady improvement in training and validation accuracy across epochs.
   - Validation accuracy closely follows training accuracy, showing no signs of overfitting.
2. **Efficiency**:
   - Model performance plateaued by epoch 10, suggesting the training process was efficient and sufficient.

### Strengths
- High generalization capability.
- Stable and reliable learning process.

### Areas for Improvement
- Explore further hyperparameter tuning.
- Add data augmentation to enhance model robustness.

---

## ğŸ›  How to Use

### 1. Prepare the Environment
Ensure you have the required dependencies installed. Use the following command to set up:

```bash
pip install -r requirements.txt
```

### 2. Run the Training Script
Train the model using the provided script:

```bash
python train.py
```

### 3. Visualize Results
Generated graphs and logs will be saved in the outputs/ directory.

ğŸŒŸ Future Work
- Hyperparameter Optimization: Fine-tune the learning rate, batch size, and architecture to further improve performance.
- Data Augmentation: Enhance the dataset using augmentation techniques to boost generalization.
- Testing: Evaluate the model on a separate test dataset to confirm final performance.

ğŸ“‚ Repository Structure

```bash
â”œâ”€â”€ data/              # Dataset
â”œâ”€â”€ outputs/           # Training logs and visualizations
â”œâ”€â”€ train.py           # Training script
â”œâ”€â”€ requirements.txt   # Dependencies
â””â”€â”€ README.md          # Project 
```
